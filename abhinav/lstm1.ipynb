{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/scratch/abkumar/anaconda/torch1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# nlp library of Pytorch\n",
    "from torchtext.legacy import data as dt\n",
    "import numpy as np\n",
    "import torchtext\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_pos = \"/cluster/home/abkumar/dataset/twitter-datasets/train_pos.txt\"\n",
    "file_path_neg = \"/cluster/home/abkumar/dataset/twitter-datasets/train_neg.txt\"\n",
    "file_path_all = \"/cluster/home/abkumar/dataset/twitter-datasets/train_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = []\n",
    "data_neg = []\n",
    "with open(file_path_pos) as f:\n",
    "    for i in f:\n",
    "        t = i.replace('<user>', '')\n",
    "        t1 = t.replace('<url>', '')\n",
    "        data_pos.append(t1)\n",
    "\n",
    "with open(file_path_neg) as f:\n",
    "    for i in f:\n",
    "        t = i.replace('<user>', '')\n",
    "        t1 = t.replace('<url>', '')\n",
    "        data_neg.append(t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "data = data_pos + data_neg\n",
    "data_labels = [1]* len(data_pos) + [-1] * len(data_neg)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    200000 non-null  object\n",
      " 1   label   200000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':data})\n",
    "df['label'] = pd.Series(data_labels)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/cluster/home/abkumar/dataset/twitter-datasets/train_all.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "def spacy_tokenize(x):\n",
    "    return [tok.text for tok in tokenizer(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = dt.Field(tokenize=spacy_tokenize, batch_first=True,include_lengths=True)\n",
    "LABEL = dt.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('text',TEXT), (\"label\", LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [' ', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "training_data = dt.TabularDataset(path=file_path_all,\n",
    "                                    format=\"csv\",\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True\n",
    "                                   )\n",
    "\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# train and validation splitting\n",
    "train_data,valid_data = training_data.split(split_ratio=0.75,\n",
    "                                            random_state=random.seed(2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabularies => (Token to integer)\n",
    "TEXT.build_vocab(train_data,\n",
    "                 min_freq=5)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text vocab: 17676\n",
      "Size of label vocab: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 75790),\n",
       " ('!', 62129),\n",
       " ('i', 60373),\n",
       " ('the', 45513),\n",
       " (',', 44949),\n",
       " ('.', 44937),\n",
       " ('to', 41818),\n",
       " ('you', 35803),\n",
       " ('(', 35200),\n",
       " ('a', 31076)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# We'll create iterators to get batches of data when we want to use them\n",
    "\"\"\"\n",
    "This BucketIterator batches the similar length of samples and reduces the need of \n",
    "padding tokens. This makes our future model more stable\n",
    "\n",
    "\"\"\"\n",
    "train_iterator,validation_iterator = dt.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        \n",
    "        # Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        \n",
    "        # LSTM layer process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True\n",
    "                           )\n",
    "        \n",
    "        # Dense layer to predict \n",
    "        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n",
    "        # Prediction activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Thanks to packing, LSTM don't see padding tokens \n",
    "        # and this makes our model better\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n",
    "        \n",
    "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Concatenating the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        \n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.sigmoid(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "NUM_HIDDEN_NODES = 128\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(SIZE_OF_VOCAB,\n",
    "                EMBEDDING_DIM,\n",
    "                NUM_HIDDEN_NODES,\n",
    "                NUM_OUTPUT_NODES,\n",
    "                NUM_LAYERS,\n",
    "                BIDIRECTION,\n",
    "                DROPOUT\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMNet(\n",
       "  (embedding): Embedding(17676, 100)\n",
       "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text,text_lengths = batch.text\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text,text_lengths).squeeze()\n",
    "        \n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions,batch.label)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = binary_accuracy(predictions,batch.label)\n",
    "        \n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text,text_lengths).squeeze()\n",
    "              \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_HIDDEN_NODES = 64\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.417 | Train Acc: 79.80%\n",
      "\t Val. Loss: 0.404 |  Val. Acc: 80.56%\n",
      "\n",
      "\tTrain Loss: 0.390 | Train Acc: 81.57%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 81.55%\n",
      "\n",
      "\tTrain Loss: 0.371 | Train Acc: 82.65%\n",
      "\t Val. Loss: 0.379 |  Val. Acc: 82.09%\n",
      "\n",
      "\tTrain Loss: 0.356 | Train Acc: 83.62%\n",
      "\t Val. Loss: 0.374 |  Val. Acc: 82.49%\n",
      "\n",
      "\tTrain Loss: 0.343 | Train Acc: 84.34%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 82.96%\n",
      "\n",
      "\tTrain Loss: 0.332 | Train Acc: 84.96%\n",
      "\t Val. Loss: 0.366 |  Val. Acc: 83.20%\n",
      "\n",
      "\tTrain Loss: 0.322 | Train Acc: 85.46%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 83.26%\n",
      "\n",
      "\tTrain Loss: 0.313 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 83.53%\n",
      "\n",
      "\tTrain Loss: 0.304 | Train Acc: 86.51%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 83.56%\n",
      "\n",
      "\tTrain Loss: 0.296 | Train Acc: 87.03%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 83.62%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUMBER = 10\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    \n",
    "    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "NUM_HIDDEN_NODES = 128\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch --  1\n",
      "\tTrain Loss: 0.463 | Train Acc: 76.74%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 80.94%\n",
      "\n",
      "Epoch --  2\n",
      "\tTrain Loss: 0.382 | Train Acc: 82.03%\n",
      "\t Val. Loss: 0.374 |  Val. Acc: 82.65%\n",
      "\n",
      "Epoch --  3\n",
      "\tTrain Loss: 0.352 | Train Acc: 83.82%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 82.99%\n",
      "\n",
      "Epoch --  4\n",
      "\tTrain Loss: 0.329 | Train Acc: 85.11%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 83.62%\n",
      "\n",
      "Epoch --  5\n",
      "\tTrain Loss: 0.310 | Train Acc: 86.18%\n",
      "\t Val. Loss: 0.361 |  Val. Acc: 83.48%\n",
      "\n",
      "Epoch --  6\n",
      "\tTrain Loss: 0.293 | Train Acc: 87.15%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 83.87%\n",
      "\n",
      "Epoch --  7\n",
      "\tTrain Loss: 0.278 | Train Acc: 88.00%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 83.51%\n",
      "\n",
      "Epoch --  8\n",
      "\tTrain Loss: 0.261 | Train Acc: 88.82%\n",
      "\t Val. Loss: 0.374 |  Val. Acc: 83.79%\n",
      "\n",
      "Epoch --  9\n",
      "\tTrain Loss: 0.245 | Train Acc: 89.59%\n",
      "\t Val. Loss: 0.387 |  Val. Acc: 83.60%\n",
      "\n",
      "Epoch --  10\n",
      "\tTrain Loss: 0.230 | Train Acc: 90.32%\n",
      "\t Val. Loss: 0.391 |  Val. Acc: 83.94%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUMBER = 10\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    \n",
    "    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(\"Epoch -- \", epoch)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5d03dd87f4ff28a18aa2d7f538041204bb8bb0840b4fb8f320c4ec6b649d9ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfe2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocess import preprocess_tweet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation, digits\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, VarianceThreshold, f_classif)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D,concatenate,Flatten,\\\n",
    "Dense,Dropout,LSTM,Masking,Bidirectional,Dropout,GRU,SimpleRNN,TimeDistributed, BatchNormalization, Activation, MaxPooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.models import Sequential,Model\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.layers.convolutional import Conv1D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca538bce",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d514eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read negative tweet lines  1250000\n",
      "read positive tweet lines  1250000\n",
      "2500000\n",
      "2500000\n"
     ]
    }
   ],
   "source": [
    "n_data = []\n",
    "p_data = []\n",
    "\n",
    "# read positive and negative tweet data\n",
    "# use own path here\n",
    "with open('/Users/louancillon/Documents/ETHZ/M2/CIL/ETH_CIL22/twitter-datasets/train_neg_full.txt', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    n_data = lines\n",
    "print('read negative tweet lines ', len(n_data))\n",
    "\n",
    "with open('/Users/louancillon/Documents/ETHZ/M2/CIL/ETH_CIL22/twitter-datasets/train_pos_full.txt', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    p_data = lines\n",
    "print('read positive tweet lines ', len(p_data))\n",
    "\n",
    "# fill all tweet data and target labels\n",
    "tweet_data = p_data + n_data\n",
    "tweet_labels = [1.0 for _ in p_data] + [0.0 for _ in n_data]\n",
    "print(len(tweet_data))\n",
    "print(len(tweet_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a8ed301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'review':tweet_data, 'sentiment':tweet_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e79dbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above function to df['text']\n",
    "#prep_tweet = df['review'].map(lambda x: preprocess_tweet(x, False))\n",
    "original = df['review'].map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "378ad923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.insert(1, 'preprocessed',prep_tweet)\n",
    "df.insert(1, 'original',original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f1c604f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>original</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "      <td>[&lt;user&gt;, i, dunno, justin, read, my, mention, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "      <td>[because, your, logic, is, so, dumb, ,, i, won...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "      <td>[\", &lt;user&gt;, just, put, casper, in, a, box, !, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "      <td>[&lt;user&gt;, &lt;user&gt;, thanks, sir, &gt;, &gt;, don't, tri...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "      <td>[visiting, my, brother, tmr, is, the, bestest,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  <user> i dunno justin read my mention or not ....   \n",
       "1  because your logic is so dumb , i won't even c...   \n",
       "2  \" <user> just put casper in a box ! \" looved t...   \n",
       "3  <user> <user> thanks sir > > don't trip lil ma...   \n",
       "4  visiting my brother tmr is the bestest birthda...   \n",
       "\n",
       "                                            original  sentiment  \n",
       "0  [<user>, i, dunno, justin, read, my, mention, ...        1.0  \n",
       "1  [because, your, logic, is, so, dumb, ,, i, won...        1.0  \n",
       "2  [\", <user>, just, put, casper, in, a, box, !, ...        1.0  \n",
       "3  [<user>, <user>, thanks, sir, >, >, don't, tri...        1.0  \n",
       "4  [visiting, my, brother, tmr, is, the, bestest,...        1.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5982c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test,Y_train, Y_test = train_test_split(df['preprocessed'], df['sentiment'], test_size=0.2, random_state = 45)\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(df['original'], df['sentiment'], test_size=0.1, random_state = 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "748d3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 554764\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(X_train, size=150, window=10, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "# save model in ASCII (word2vec) format\n",
    "model.save(\"W2V_150_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b74b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 554764 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Set Maximum number of words to be embedded\n",
    "NUM_WORDS = 25000\n",
    "\n",
    "# Define/Load Tokenize text function\n",
    "#tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,lower=True)\n",
    "\n",
    "# Fit the function on the text\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Count number of unique tokens\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "852fe388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and val to sequence\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82ef84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (2250000, 50)\n",
      "Shape of label train and validation tensor: (2250000,)\n"
     ]
    }
   ],
   "source": [
    "# Limit size of train/val to 50 and pad the sequence\n",
    "X_train = pad_sequences(sequences_train,maxlen=50)\n",
    "\n",
    "# Convert target to array\n",
    "Y_train = np.asarray(Y_train)\n",
    "\n",
    "# Printing shape\n",
    "print('Shape of X train and X validation tensor:', X_train.shape)\n",
    "print('Shape of label train and validation tensor:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f5bcc",
   "metadata": {},
   "source": [
    "## Embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad50502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec.load(\"W2V_150_10\")\n",
    "word_vectors = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab8595d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=150\n",
    "vocabulary_size=min(len(word_index)+1,(NUM_WORDS))\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        vec = np.zeros(EMBEDDING_DIM)\n",
    "        if word in bad_words:\n",
    "            vec = word_vectors['fuck']\n",
    "        embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5138aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dfine Embedding function using the embedding_matrix\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb105e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4]\n",
    "num_filters = 512\n",
    "drop = 0.2\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((2*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "conc = Dense(1)(dropout)\n",
    "output = Dense(units=1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.001))(conc)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9b37455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 150)      3750000     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 50, 150, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 48, 1, 512)   230912      reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 47, 1, 512)   307712      reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 1, 1, 512)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 1, 1, 512)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2, 1, 512)    0           max_pooling2d_10[0][0]           \n",
      "                                                                 max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1024)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            1025        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            2           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,289,651\n",
      "Trainable params: 4,289,651\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file_path =  \"modelstack1024.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=5, verbose=1)\n",
    "red = ReduceLROnPlateau(monitor=\"acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint,early, red]\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(0.001), loss=losses.binary_crossentropy, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28781ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2198/2198 [==============================] - 2248s 1s/step - loss: 0.4167 - acc: 0.8313\n",
      "Epoch 2/100\n",
      "2198/2198 [==============================] - 2211s 1s/step - loss: 0.3831 - acc: 0.8444\n",
      "Epoch 3/100\n",
      "2198/2198 [==============================] - 2208s 1s/step - loss: 0.3713 - acc: 0.8484\n",
      "Epoch 4/100\n",
      "2198/2198 [==============================] - 2207s 1s/step - loss: 0.3641 - acc: 0.8512\n",
      "Epoch 5/100\n",
      "2198/2198 [==============================] - 2203s 1s/step - loss: 0.3596 - acc: 0.8527\n",
      "Epoch 6/100\n",
      "2198/2198 [==============================] - 2193s 997ms/step - loss: 0.3563 - acc: 0.8541\n",
      "Epoch 7/100\n",
      "2198/2198 [==============================] - 2184s 994ms/step - loss: 0.3542 - acc: 0.8550\n",
      "Epoch 8/100\n",
      "2198/2198 [==============================] - 2175s 990ms/step - loss: 0.3516 - acc: 0.8561\n",
      "Epoch 9/100\n",
      "2198/2198 [==============================] - 2173s 989ms/step - loss: 0.3496 - acc: 0.8571\n",
      "Epoch 10/100\n",
      "2198/2198 [==============================] - 2168s 986ms/step - loss: 0.3480 - acc: 0.8578\n",
      "Epoch 11/100\n",
      "2198/2198 [==============================] - 2168s 986ms/step - loss: 0.3468 - acc: 0.8587\n",
      "Epoch 12/100\n",
      "2198/2198 [==============================] - 2166s 985ms/step - loss: 0.3451 - acc: 0.8593\n",
      "Epoch 13/100\n",
      "2198/2198 [==============================] - 2164s 984ms/step - loss: 0.3438 - acc: 0.8598\n",
      "Epoch 14/100\n",
      "2198/2198 [==============================] - 2159s 982ms/step - loss: 0.3429 - acc: 0.8609\n",
      "Epoch 15/100\n",
      " 397/2198 [====>.........................] - ETA: 29:27 - loss: 0.3412 - acc: 0.8614"
     ]
    }
   ],
   "source": [
    "# Compiling Model using optimizer\n",
    "#opt = optimizers.Adam(0.001)\n",
    "#model.compile(loss='binary_crossentropy',optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "# Fitting Model to the data\n",
    "#callbacks = [EarlyStopping(monitor='accuracy')]\n",
    "model.fit(X_train, Y_train, batch_size=1024, epochs=100, verbose=1)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf541fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert train and val to sequence\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "# Limit size of train/val to 50 and pad the sequence\n",
    "X_test = pad_sequences(sequences_test,maxlen=50)\n",
    "# Convert target to array\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f268f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d60bf10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and val to sequence\n",
    "sequences_test = tokenizer.texts_to_sequences(test['test'])\n",
    "# Limit size of train/val to 50 and pad the sequence\n",
    "X_test = pad_sequences(sequences_test,maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fb328a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7af9d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [1 if test_pred[i]>0.5 else 0 for i in range(len(test_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fec70c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelstackembfull0.008.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b48bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"modelstackembfull0.008.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ece6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd412c5e",
   "metadata": {},
   "source": [
    "### Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da158a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"embedding_word2vec_full0.1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "707605db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 107402\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(X_train, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "# save model in ASCII (word2vec) format\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bd60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "model = Word2Vec.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac8b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(model):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(model.wv.vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 42, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    i=0\n",
    "    for word in model.wv.vocab.keys():\n",
    "        #weight_matrix[i] = model[word].wv.__getitem__()\n",
    "        weight_matrix[i] = model.wv.__getitem__(word)\n",
    "        i+=1\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7070ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedd_concatenate(model,data, original):\n",
    "    x = []\n",
    "    maxwords = max([len(original[i]) for i in range(len(original))])\n",
    "    for index in range(len(data)):\n",
    "        # remove out-of-vocabulary words\n",
    "        doc = [word for word in data.iloc[index] if word in model.wv.vocab.keys()]\n",
    "        \n",
    "        if not doc:\n",
    "            # append zero vector\n",
    "            x.append(np.zeros((maxwords,100)))\n",
    "\n",
    "        else:\n",
    "            # append the average vector for each sentence\n",
    "            sentence = np.array(model.wv[doc])\n",
    "            #x.append(np.array(model[doc], dtype=object))\n",
    "            #print(len(model[doc]))\n",
    "            if len(model.wv.__getitem__(doc)) < maxwords:\n",
    "                sentence = np.append(sentence, np.zeros(((maxwords-len(model.wv.__getitem__(doc))),100)), axis=0)\n",
    "                #print(len(sentence))\n",
    "            x.append(np.array(sentence))\n",
    "            \n",
    "        \n",
    "    X = np.array(x)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de45122",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sentence_embedd_concatenate(model, X_train, original)\n",
    "x_test = sentence_embedd_concatenate(model, X_test, original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864566f5",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model5 = Sequential()\n",
    "#model5.add(embedding_layer)\n",
    "model5.add(Conv1D(filters=256, kernel_size=5, activation='relu', input_shape=(64, 100)))\n",
    "model5.add(MaxPooling1D(pool_size=2))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Dense(64,activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "file_path =  \"model_emb.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=5, verbose=1)\n",
    "red = ReduceLROnPlateau(monitor=\"acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint,early, red]\n",
    "\n",
    "model5.compile(optimizer=optimizers.Adam(0.001), loss=losses.binary_crossentropy, metrics=['acc'])\n",
    "model5.summary()\n",
    "\n",
    "\n",
    "# fit network\n",
    "model5.fit(x_train, Y_train, epochs=4, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model5.evaluate(x_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "model5.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb768e",
   "metadata": {},
   "source": [
    "## Load models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0395d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.505000\n"
     ]
    }
   ],
   "source": [
    "## Same model on the original data without preprocessing 80%training : 82.5 accuracy \n",
    "## Same model on the original data without preprocessing 90%training : 83.02 accuracy \n",
    "\n",
    "model_stack = models.load_model(\"modelstack256x2.h5\")\n",
    "loss, acc = model_stack.evaluate(x_test, Y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d492e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
